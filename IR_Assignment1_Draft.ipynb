{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import json \n",
    "import math\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = 'https://raw.githubusercontent.com/emorynlp/character-mining/master/json/friends_season_09.json'\n",
    "handle = requests.get(json_file)\n",
    "season = json.loads(handle.text) ##extracting the json file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = season['episodes'] ##conatins the episodes object which has each scene and utterance within\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue = {} ## dictionary that stores all the dialogues, grouped by scene id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_scenes = 0 ## total number of scenes in all the episodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filling the dialogue {} dictionary \n",
    "for episode in episodes:\n",
    "    scenes = episode['scenes']\n",
    "    for scene in scenes:\n",
    "        scene_id = scene['scene_id']\n",
    "        num_scenes += 1 ## calculating the total number of scenes in the whole season\n",
    "        utterances = scene['utterances']\n",
    "        for utterance in utterances: \n",
    "           ## for i in range(len(utterance)):\n",
    "                transcript = utterance['transcript']\n",
    "                if scene_id not in dialogue.keys():\n",
    "                    dialogue[scene_id] = transcript\n",
    "                    dialogue[scene_id] += '\\n'\n",
    "                else:\n",
    "                    dialogue[scene_id] += transcript\n",
    "                    dialogue[scene_id] += '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = dict.fromkeys(dialogue.keys(), []) ## this dictionary contains normalized tokens grouped by scene id\n",
    "for key in dialogue: \n",
    "    tokens = tokenizer.tokenize(dialogue[key])\n",
    "    stopset = set(stopwords.words('english'))\n",
    "    filtered_sentence = [w for w in tokens if not w in stopset]\n",
    "    stemmed_sentence = [stemmer.stem(w) for w in filtered_sentence]\n",
    "    lemmatized_sentence = [lemmatizer.lemmatize(w) for w in stemmed_sentence]\n",
    "    docs[key] = lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating the inverted index\n",
    "## the keys are the tokens, the values are the scene is occurs in and the frequency in each scene\n",
    "inverted_index = {} \n",
    "for key in docs:\n",
    "    for word in docs[key]:\n",
    "        if word not in inverted_index.keys():\n",
    "            sc = {}\n",
    "            sc[key] = 1\n",
    "            inverted_index[word] = sc\n",
    "        else:\n",
    "            if key not in inverted_index[word].keys():\n",
    "                inverted_index[word][key] = 1\n",
    "            else:\n",
    "                inverted_index[word][key] += 1             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictio={} ## number of unique words in each scene\n",
    "## keys are the scene_ids, the values are the number of unique normalized words in each scene\n",
    "for key in docs:\n",
    "    set1 = set(docs[key])\n",
    "    length=len(set1)\n",
    "    dictio[key]=length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(inverted_index) ## number of total unique words in all scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a dialogue:\n",
      "how you doin\n",
      "1.Natural Term Frequency\n",
      "2.Logarithm Term Frequency\n",
      "3.Augmented Term Frequency\n",
      "4.Boolean Term Frequency\n",
      "\n",
      "Choose your scoring scheme:\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "query = input('Enter a dialogue:\\n')\n",
    "print(\"1.Natural Term Frequency\\n2.Logarithm Term Frequency\\n3.Augmented Term Frequency\\n4.Boolean Term Frequency\\n\")\n",
    "case = input(\"Choose your scoring scheme:\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "## collecting the normalized tokens for the query\n",
    "tokens = tokenizer.tokenize(query)\n",
    "stopset = set(stopwords.words('english'))\n",
    "filtered_sentence = [w for w in tokens if not w in stopset]\n",
    "stemmed_sentence = [stemmer.stem(w) for w in filtered_sentence]\n",
    "lemmatized_sentence = [lemmatizer.lemmatize(w) for w in stemmed_sentence]\n",
    "query = lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_freq(term,doc): ## term frequency\n",
    "    return inverted_index[term][doc]\n",
    "\n",
    "def doc_freq(term): ## document frequency \n",
    "    return len(inverted_index[term])\n",
    "\n",
    "def inverted_doc_freq(term): ##inverted document frequency\n",
    "    return num_scenes/doc_freq(term)\n",
    "\n",
    "def log_term_freq(term,doc): ##logarithmic term frequency\n",
    "    return 1+math.log(term_freq(term,doc))\n",
    "\n",
    "def aug_term_freq(term,doc,_max): ##augmented term frequency\n",
    "     return 0.5+(0.5*term_freq(term,doc)/max_)\n",
    "    \n",
    "def bool_term_freq(term,doc): ## boolean term frequency \n",
    "    if term_freq(term,doc)>0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the maximum term frequency for any term in the query \n",
    "max_=0\n",
    "for term in query:\n",
    "    if term not in inverted_index.keys():\n",
    "        continue\n",
    "    for key in inverted_index[term]:\n",
    "        if term_freq(term, key)>max_:\n",
    "            max_= term_freq(term,key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(term,doc): ##function to return tf-idf score based on chosen scoring scheme\n",
    "    if(int(case)==1):\n",
    "        return term_freq(term,doc)*inverted_doc_freq(term)\n",
    "    if(int(case)==2):\n",
    "        return log_term_freq(term,doc)*inverted_doc_freq(term)\n",
    "    if(int(case)==3):\n",
    "        return aug_term_freq(term,doc,_max)*inverted_doc_freq(term)\n",
    "    if(int(case)==4):\n",
    "        return bool_term_freq(term,doc)*inverted_doc_freq(term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "## score_matrix has keys as scene_id; values are pairs of (term frequency, score based on chosen function)\n",
    "score_matrix = {}\n",
    "\n",
    "for term in query:\n",
    "    if term not in inverted_index.keys():\n",
    "        continue\n",
    "    for key in inverted_index[term]:\n",
    "        if key not in score_matrix:\n",
    "            score_matrix[key] = []\n",
    "            score_matrix[key].append(1)\n",
    "            score_matrix[key].append(score(term, key))\n",
    "        else:\n",
    "            score_matrix[key][1] += score(term, key)\n",
    "            score_matrix[key][0] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices = sorted(score_matrix.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the top 10 scenes related to the given query\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-121-fcb8de7fd34f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'These are the top 10 scenes related to the given query'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\". \"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print('These are the top 10 scenes related to the given query')\n",
    "for i in range(10):\n",
    "    print(str(i+1) + \". \"+ sorted_indices[i][0] + \" \" + str(sorted_indices[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
