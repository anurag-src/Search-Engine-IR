{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import json \n",
    "import math\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = 'https://raw.githubusercontent.com/emorynlp/character-mining/master/json/friends_season_01.json'\n",
    "handle = requests.get(json_file)\n",
    "season_1 = json.loads(handle.text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends_data = [] ## the master list that will contain the complete dataset\n",
    "## each element of the list will be the complete dictionary that contains that particular seasons dataset, indexed with 0 \n",
    "## friends_data[0] is the season 1 dataset and so on \n",
    "friends_data.append(season_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for season_index in range(2, 11): ##merging the dataset of all seasons \n",
    "    season_index = '0%d'%season_index if season_index <10 else str(season_index)\n",
    "    \n",
    "    json_url = 'https://raw.githubusercontent.com/emorynlp/character-mining/master/json/friends_season_%s.json'%season_index\n",
    "    request = requests.get(json_url)\n",
    "    curr_season = json.loads(request.text) ## the current season \n",
    "    friends_data.append(curr_season)\n",
    "\n",
    "## merging complete "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue = {} ## dictionary that stores all the dialogues, grouped by scene id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_scenes = 0 ## total number of scenes in all the episodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filling in the dialogue dictionary \n",
    "for season in friends_data:\n",
    "    episodes = season['episodes']\n",
    "    for episode in episodes:\n",
    "        scenes = episode['scenes']\n",
    "        for scene in scenes:\n",
    "            scene_id = scene['scene_id']\n",
    "            num_scenes += 1 ## calculating the total number of scenes in the whole show\n",
    "            utterances = scene['utterances']\n",
    "            for utterance in utterances: \n",
    "                transcript = utterance['transcript']\n",
    "                if scene_id not in dialogue.keys():\n",
    "                    dialogue[scene_id] = transcript\n",
    "                    dialogue[scene_id] += '\\n'\n",
    "                else:\n",
    "                    dialogue[scene_id] += transcript\n",
    "                    dialogue[scene_id] += '\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = dict.fromkeys(dialogue.keys(), []) ## this dictionary contains normalized tokens grouped by scene id\n",
    "for key in dialogue: \n",
    "    tokens = tokenizer.tokenize(dialogue[key])\n",
    "    stopset = set(stopwords.words('english'))\n",
    "    filtered_sentence = [w for w in tokens if not w in stopset]\n",
    "    stemmed_sentence = [stemmer.stem(w) for w in filtered_sentence]\n",
    "    lemmatized_sentence = [lemmatizer.lemmatize(w) for w in stemmed_sentence]\n",
    "    docs[key] = lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating the inverted index\n",
    "## the keys are the tokens, the values are the scene is occurs in and the frequency in each scene\n",
    "inverted_index = {} \n",
    "for key in docs:\n",
    "    for word in docs[key]:\n",
    "        if word not in inverted_index.keys():\n",
    "            sc = {}\n",
    "            sc[key] = 1\n",
    "            inverted_index[word] = sc\n",
    "        else:\n",
    "            if key not in inverted_index[word].keys():\n",
    "                inverted_index[word][key] = 1\n",
    "            else:\n",
    "                inverted_index[word][key] += 1       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a dialogue:\n",
      "Hey and look he brought flowers. Thanks Ross, but I'm really more of a candy guy.\n",
      "1.Natural Term Frequency\n",
      "2.Logarithm Term Frequency\n",
      "3.Augmented Term Frequency\n",
      "4.Boolean Term Frequency\n",
      "\n",
      "Choose your scoring scheme:\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "query = input('Enter a dialogue:\\n')\n",
    "print(\"1.Natural Term Frequency\\n2.Logarithm Term Frequency\\n3.Augmented Term Frequency\\n4.Boolean Term Frequency\\n\")\n",
    "case = input(\"Choose your scoring scheme:\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## collecting the normalized tokens for the query\n",
    "tokens = tokenizer.tokenize(query)\n",
    "stopset = set(stopwords.words('english'))\n",
    "filtered_sentence = [w for w in tokens if not w in stopset]\n",
    "stemmed_sentence = [stemmer.stem(w) for w in filtered_sentence]\n",
    "lemmatized_sentence = [lemmatizer.lemmatize(w) for w in stemmed_sentence]\n",
    "query = lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_freq(term,doc): ## term frequency\n",
    "    return inverted_index[term][doc]\n",
    "\n",
    "def doc_freq(term): ## document frequency \n",
    "    return len(inverted_index[term])\n",
    "\n",
    "def inverted_doc_freq(term): ##inverted document frequency\n",
    "    return num_scenes/doc_freq(term)\n",
    "\n",
    "def log_term_freq(term,doc): ##logarithmic term frequency\n",
    "    return 1+math.log(term_freq(term,doc))\n",
    "\n",
    "def aug_term_freq(term,doc,_max): ##augmented term frequency\n",
    "     return 0.5+(0.5*term_freq(term,doc)/max_)\n",
    "    \n",
    "def bool_term_freq(term,doc): ## boolean term frequency \n",
    "    if term_freq(term,doc)>0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the maximum term frequency for any term in the query \n",
    "max_=0\n",
    "for term in query:\n",
    "    if term not in inverted_index.keys():\n",
    "        continue\n",
    "    for key in inverted_index[term]:\n",
    "        if term_freq(term, key)>max_:\n",
    "            max_= term_freq(term,key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(term,doc): ##function to return tf-idf score based on chosen scoring scheme\n",
    "    if(int(case)==1):\n",
    "        return term_freq(term,doc)*inverted_doc_freq(term)\n",
    "    if(int(case)==2):\n",
    "        return log_term_freq(term,doc)*inverted_doc_freq(term)\n",
    "    if(int(case)==3):\n",
    "        return aug_term_freq(term,doc,_max)*inverted_doc_freq(term)\n",
    "    if(int(case)==4):\n",
    "        return bool_term_freq(term,doc)*inverted_doc_freq(term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## score_matrix has keys as scene_id; values are pairs of (term frequency, score based on chosen function)\n",
    "score_matrix = {}\n",
    "\n",
    "for term in query:\n",
    "    if term not in inverted_index.keys():\n",
    "        continue\n",
    "    for key in inverted_index[term]:\n",
    "        if key not in score_matrix:\n",
    "            score_matrix[key] = []\n",
    "            score_matrix[key].append(1)\n",
    "            score_matrix[key].append(score(term, key))\n",
    "        else:\n",
    "            score_matrix[key][1] += score(term, key)\n",
    "            score_matrix[key][0] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices = sorted(score_matrix.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the top 10 scenes related to the given query\n",
      "1. s09_e01_c01 [10, 273.2812651622011]\n",
      "2. s06_e20_c08 [9, 153.31577401610232]\n",
      "3. s08_e06_c03 [8, 731.0548068869847]\n",
      "4. s09_e08_c01 [8, 304.1624403564283]\n",
      "5. s05_e13_c06 [8, 223.33129991164614]\n",
      "6. s08_e18_c04 [8, 163.4194732990083]\n",
      "7. s09_e18_c02 [8, 149.3433054550322]\n",
      "8. s05_e08_c09 [8, 146.40220106716683]\n",
      "9. s10_e04_c02 [8, 101.2351565401503]\n",
      "10. s05_e15_c03 [8, 76.77083561454714]\n"
     ]
    }
   ],
   "source": [
    "print('These are the top 10 scenes related to the given query')\n",
    "for i in range(10):\n",
    "    print(str(i+1) + \". \"+ sorted_indices[i][0] + \" \" + str(sorted_indices[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
